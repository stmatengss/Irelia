/**
  * Created by stmatengss on 16-4-13.
  */

import org.apache.spark.rdd._
import org.apache.spark.HashPartitioner
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.graphx._

import scala.collection

//val edges=sc.makeRDD(Array((8,1),(8,5),(9,8),(8,7),(7,3),(3,2),(6,3),(6,4)))
//val edgesLoad=sc.makeRDD(Array((1L,2L),(2L,3L),(2L,4L),(4L,5L),(6L,7L)))

object CT {

  //  val que=scala.collection.mutable.Queue[RDD[(Long,Long)]]();
  //  val resPro=scala.collection.mutable.ArrayBuffer[(Long,Long)]();
  val SUM: Int = 0;
  val RES: Int = 1;
  val CHECK: Int = 2;

  def countingReplica(e: Array[(Long, Array[Long])], center: Array[Long], nonCenter: Array[Long]): Iterator[((Long,Long),Int)]= {
	val adList = e;
	var res:Long = 0;
	val trans = adList.map(x => x._1).zipWithIndex.toMap;
	var root: Int = 0;
	var son: Int = 0;
	var count: Long = 0;

/*	for (u <- (nonCenters)) {
	  root = trans(u);

	  for (v <- adList(root)._2 if v > u) {
		for (w <- adList(root)._2 if w > u;
			 if adList(trans(v))._2.exists(_ == w)) {
		  count += 1;
		}
	  }
	  resVertex += ((u, count));
	}*/
		//a new method to calculate
	for(u<-nonCenter++center if trans.contains(u)) {
	  root = trans(u);
	  println("U", u)
	  count = 0;
	  for (v <- adList(root)._2 if trans.contains(v)) {
		son = trans(v);
		count += adList(son)._2.intersect(adList(root)._2).length;
//		for(w <- adList(son)._2 if isCenter(w))check+=(((u, w),CHECK));
	  }
	  res += count;
	}
	println("Res=="+res.toString);
	Iterator(((res,res), SUM))
  }

  def update(part: Iterator[((Long, Long), (Long, Long))]): Iterator[((Long, Long),Int)] = {
	val triplets = part.toArray;
	val check : Array[((Long, Long), Int)] = null;

	println("triplets:",triplets.foreach(print));
	val outDegrees = triplets.flatMap({ case (x, y) => Iterator(x, y) }).distinct.sortWith({
	  case (x, y) => (x._1 < y._1)
	});
		println("outDefrees:",outDegrees.foreach(print));
	val edges=triplets.map(x => (x._1._1, x._2._1));
	val allEdges=edges.flatMap({
	  case (src, dst) => Iterator((src, dst), (dst, src))
	}).groupBy(_._1);
	// TODO
	val partOutDegrees = allEdges.map({
	  case (x, y) => (x, y.length)
	}).toArray.sortWith({
	  case (x, y) => (x._1 < y._1)
	});
	val centers = scala.collection.mutable.ArrayBuffer[(Long)]();
	val nonCenters = scala.collection.mutable.ArrayBuffer[(Long)]();
		println("partOutDegrees:",partOutDegrees.foreach(print));
	// or Join?  If the partitioner will make the RDD in order
	outDegrees.zip(partOutDegrees).foreach({
		case (pro, now) => {
		  if (pro._2 == now._2) {
			nonCenters += (pro._1);
		  } else {
			centers += (pro._1);
		  }
		}
	  }
	)
	val isCenter=centers.toSet;

	val less=edges.filter({
	  case (x,y)=>{isCenter(x)&&isCenter(y)}
	}).map({
	  case(src,dst)=>
		if(src<dst) ((src, dst),RES)
		else ((dst, src),RES)
	});

	val adList= edges.map({
	  case (x,y)=>{
		x > y match{   //TODO
		  case true=>(y,x);
		  case _=>(x,y);
		}
	  }
	}).groupBy(_._1).map(x => (x._1, x._2.map(_._2)));
	countingReplica(adList.toArray, centers.toArray, nonCenters.toArray)++less.toIterator;
  }

  def run(edgesLoad: RDD[(Long, Long)], cluster: Long, directed: Boolean, th:Int ): Long= {

	val LOWER_BOUND: Long = edgesLoad.count / cluster;
	var edges: RDD[(Long, Long)] = null;
	var check: RDD[(Long, Long)] = null;
	var resTmp: RDD[((Long, Long), Int)] = null;
	var clusterNum: Long = cluster;
	var resPro: Long=0;
	var resCheck: Long=0;
	var res:Long=0;
	var triplets: RDD[((Long, Long), (Long, Long))]=null;

	if (directed == true) {
	  edges = edgesLoad.filter({
		case (x, y) => {x < y}
	  })
	} else {
	  edges = edgesLoad
	}
	//	println("clusterNum",clusterNum);
	while (clusterNum >= 1) {
//	  edges = edges.partitionBy(new HashPartitioner(clusterNum.toInt));
	  //	  clusterNum=clusterNum/2;
	  //	  println("clusterNum",clusterNum);
	  val outDegrees: RDD[(Long, Long)] = edges.flatMap({
		case (src, dst) => Array((src.toLong, 1L), (dst.toLong, 1L))
	  }
	  ).reduceByKey(_ + _);
	  //	  println("edges:",edges.foreach(println(_)))
	  triplets = edges.join(outDegrees).map(
		{ case (x, (y, n)) => (y, (x, n)) }
	  ).join(outDegrees).map(
		{ case (x, (y, n)) => ((x, n), y) }
	  )
	  triplets = triplets.map({
		case ((src, srcAttr),(dst, dstAttr)) => {
		  if(srcAttr > dstAttr) ((dst, dstAttr),(src, srcAttr))
		  else ((src, srcAttr),(dst, dstAttr))
		}
	  });
	  triplets = triplets.partitionBy(new shrunk.PowerLyraParitioner(clusterNum.toInt,th));
//	  println("triplets:",triplets.foreach(println(_)))
	  resTmp = triplets.mapPartitions(update, true);
	  resTmp.cache().count()
	  edges = resTmp.filter({
		case (x, y) => {y == RES}
	  }).map(_._1);
	  resPro = resTmp.filter({
		case (x, y) => { y == SUM}
	  }).keys.keys.reduce(_+_);
	  check = resTmp.filter({
		case (x,y) => { y == CHECK}
	  }).map(_._1);
	  resCheck = edges.intersection(check).count;
	  //TODO more profermance
	  println("edges:",edges.foreach(print))
	  println("resCheck:",check.foreach(print))
	  println("resPro:",resPro)
	  println("resCheck:",resCheck)
	  res+=resPro+resCheck;
	  if (clusterNum == 1) clusterNum = 0;
	  else clusterNum = Math.max(1, Math.min(clusterNum, edges.count / LOWER_BOUND));
	  println("ClusterNum=", clusterNum, edges.count());
	  println("edges:",edges.foreach(print(_)))
	  //println("resPro:",resPro.foreach(println(_)))
	}
	//	println("edges::",edges.foreach(print(_)));
	//	println("Length:",que.length)
	res
  }
}


